{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3952946,"sourceType":"datasetVersion","datasetId":1554380}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":13206.847884,"end_time":"2024-02-24T18:40:17.19732","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-24T15:00:10.349436","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-02-24T13:29:38.179913Z","iopub.status.busy":"2024-02-24T13:29:38.179546Z","iopub.status.idle":"2024-02-24T13:29:38.185303Z","shell.execute_reply":"2024-02-24T13:29:38.184218Z","shell.execute_reply.started":"2024-02-24T13:29:38.179884Z"},"papermill":{"duration":0.013795,"end_time":"2024-02-24T15:00:13.084174","exception":false,"start_time":"2024-02-24T15:00:13.070379","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Readme\n--------------------------------------","metadata":{}},{"cell_type":"markdown","source":"\n\n## Introduction\n\nIn this study, we analyze a dataset consisting of 90 distinct animal images. Our objective is to explore various classification tasks using this dataset. Initially, we will organize the data for one-vs-rest classification as binary classification, and addressing a 5-class classification challenge. To assess the efficacy of these two classification model, we will employ classification matrices for comprehensive evaluation.\n\n## Classification Techniques\n\n### One-vs-Rest Classification: \nInitially, we organize the dataset to facilitate one-vs-rest classification. This approach involves training a separate classifier for each class while treating all other classes as a single class. Consequently, we iteratively train multiple classifiers, each distinguishing between one class and the rest.\n\n### 5-Class Classification:\nFinally, we address a more complex classification scenario involving five distinct classes. This setup requires the development of a classification model capable of accurately assigning each input image to one of the five predefined classes.\n\n## Model\nThe dataset underwent analysis utilizing three distinct architectures: CustomNN with attention mechanism, MobileNet, and EfficientNet. Across all cases examined, the test accuracy consistently surpassed the remarkable threshold of 99.5%.\n\n## Evaluation\n\nTo gauge the effectiveness of each classification model, we employ classification matrices. These matrices provide a comprehensive overview of the model's performance by presenting key metrics such as accuracy, and precision, recall, and F1-score can be calculated by using the printed confusion matrix for each class. By analyzing these metrics, we can identify the strengths and weaknesses of each classification approach, enabling informed decision-making regarding model selection and refinement.\n\n-------------------------------------------\n# Libraries","metadata":{}},{"cell_type":"markdown","source":"Used for Folder Structure\n","metadata":{"papermill":{"duration":0.013115,"end_time":"2024-02-24T15:00:13.13708","exception":false,"start_time":"2024-02-24T15:00:13.123965","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os \nfrom PIL import Image\nimport shutil\nimport random","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:13.165736Z","iopub.status.busy":"2024-02-24T15:00:13.165466Z","iopub.status.idle":"2024-02-24T15:00:13.175459Z","shell.execute_reply":"2024-02-24T15:00:13.17477Z"},"papermill":{"duration":0.025915,"end_time":"2024-02-24T15:00:13.177269","exception":false,"start_time":"2024-02-24T15:00:13.151354","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Used for DL and Visualisation","metadata":{"papermill":{"duration":0.013184,"end_time":"2024-02-24T15:00:13.203833","exception":false,"start_time":"2024-02-24T15:00:13.190649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:13.231595Z","iopub.status.busy":"2024-02-24T15:00:13.231339Z","iopub.status.idle":"2024-02-24T15:00:21.455572Z","shell.execute_reply":"2024-02-24T15:00:21.454763Z"},"papermill":{"duration":8.240679,"end_time":"2024-02-24T15:00:21.457898","exception":false,"start_time":"2024-02-24T15:00:13.217219","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Folder Structure","metadata":{"papermill":{"duration":0.013325,"end_time":"2024-02-24T15:00:21.485108","exception":false,"start_time":"2024-02-24T15:00:21.471783","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Study of Image type and folder structure for further analysis","metadata":{}},{"cell_type":"code","source":"# The folder is contained in this directory\nanimal_dir = \"/kaggle/input/animal-image-dataset-90-different-animals/animals/animals\"","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:21.515137Z","iopub.status.busy":"2024-02-24T15:00:21.514698Z","iopub.status.idle":"2024-02-24T15:00:21.518837Z","shell.execute_reply":"2024-02-24T15:00:21.518116Z"},"papermill":{"duration":0.021066,"end_time":"2024-02-24T15:00:21.520772","exception":false,"start_time":"2024-02-24T15:00:21.499706","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlen(os.listdir(animal_dir))","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:21.549587Z","iopub.status.busy":"2024-02-24T15:00:21.549156Z","iopub.status.idle":"2024-02-24T15:00:21.594974Z","shell.execute_reply":"2024-02-24T15:00:21.594104Z"},"papermill":{"duration":0.062547,"end_time":"2024-02-24T15:00:21.596924","exception":false,"start_time":"2024-02-24T15:00:21.534377","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/animal-image-dataset-90-different-animals/name of the animals.txt\", 'r') as f:\n    animal_info = f.read()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-24T15:00:21.625145Z","iopub.status.busy":"2024-02-24T15:00:21.624864Z","iopub.status.idle":"2024-02-24T15:00:21.631517Z","shell.execute_reply":"2024-02-24T15:00:21.630669Z"},"papermill":{"duration":0.022799,"end_time":"2024-02-24T15:00:21.633421","exception":false,"start_time":"2024-02-24T15:00:21.610622","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(animal_info.split())","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:21.661783Z","iopub.status.busy":"2024-02-24T15:00:21.661509Z","iopub.status.idle":"2024-02-24T15:00:21.665768Z","shell.execute_reply":"2024-02-24T15:00:21.664863Z"},"papermill":{"duration":0.021101,"end_time":"2024-02-24T15:00:21.668129","exception":false,"start_time":"2024-02-24T15:00:21.647028","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(animal_info.split())","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:21.696284Z","iopub.status.busy":"2024-02-24T15:00:21.69599Z","iopub.status.idle":"2024-02-24T15:00:21.701096Z","shell.execute_reply":"2024-02-24T15:00:21.700275Z"},"papermill":{"duration":0.021446,"end_time":"2024-02-24T15:00:21.703023","exception":false,"start_time":"2024-02-24T15:00:21.681577","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-02-24T13:29:39.136628Z","iopub.status.busy":"2024-02-24T13:29:39.136349Z","iopub.status.idle":"2024-02-24T13:29:39.140556Z","shell.execute_reply":"2024-02-24T13:29:39.139664Z","shell.execute_reply.started":"2024-02-24T13:29:39.136605Z"},"papermill":{"duration":0.013722,"end_time":"2024-02-24T15:00:21.730558","exception":false,"start_time":"2024-02-24T15:00:21.716836","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nanimal_names = {} \nanimal_directories = os.listdir(animal_dir)\n\nfor animal_name in animal_directories:\n    animal_path = os.path.join(animal_dir, animal_name)\n    num_images = len(os.listdir(animal_path))\n    animal_names[animal_name] = num_images","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:21.759096Z","iopub.status.busy":"2024-02-24T15:00:21.758856Z","iopub.status.idle":"2024-02-24T15:00:22.798789Z","shell.execute_reply":"2024-02-24T15:00:22.797776Z"},"papermill":{"duration":1.056845,"end_time":"2024-02-24T15:00:22.801154","exception":false,"start_time":"2024-02-24T15:00:21.744309","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"animal_names","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:22.831163Z","iopub.status.busy":"2024-02-24T15:00:22.830878Z","iopub.status.idle":"2024-02-24T15:00:22.839482Z","shell.execute_reply":"2024-02-24T15:00:22.838595Z"},"papermill":{"duration":0.025378,"end_time":"2024-02-24T15:00:22.841479","exception":false,"start_time":"2024-02-24T15:00:22.816101","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-02-24T13:29:40.798229Z","iopub.status.busy":"2024-02-24T13:29:40.797856Z","iopub.status.idle":"2024-02-24T13:29:40.803419Z","shell.execute_reply":"2024-02-24T13:29:40.802502Z","shell.execute_reply.started":"2024-02-24T13:29:40.798198Z"},"papermill":{"duration":0.013774,"end_time":"2024-02-24T15:00:22.869324","exception":false,"start_time":"2024-02-24T15:00:22.85555","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimage_path = \"/kaggle/input/animal-image-dataset-90-different-animals/animals/animals/antelope/02f4b3be2d.jpg\"\nimage = Image.open(image_path)\nwidth, height = image.size\nprint(f\"width: {width}, height: {height}\")","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:22.898592Z","iopub.status.busy":"2024-02-24T15:00:22.898312Z","iopub.status.idle":"2024-02-24T15:00:22.913338Z","shell.execute_reply":"2024-02-24T15:00:22.912425Z"},"papermill":{"duration":0.031733,"end_time":"2024-02-24T15:00:22.915207","exception":false,"start_time":"2024-02-24T15:00:22.883474","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **One vs Rest Classification**","metadata":{"papermill":{"duration":0.013935,"end_time":"2024-02-24T15:00:22.943478","exception":false,"start_time":"2024-02-24T15:00:22.929543","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This dataset class retrieves all images from the folder at the `ith` index, as well as `10%` of the images from each of the other folders. This approach helps to conserve memory and end potential train and test problems arising from significant discrepancies in image numbers across folders.\n\n\n```\nNote: Earlier the dataset took `ith` folder (60) images, and (5340) images in the rest folder, but this caused errors so, in the final version of the code this issue was solved using this method.\n```","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/input/animal-image-dataset-90-different-animals/animals/animals\"","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:22.972815Z","iopub.status.busy":"2024-02-24T15:00:22.972131Z","iopub.status.idle":"2024-02-24T15:00:22.976062Z","shell.execute_reply":"2024-02-24T15:00:22.975215Z"},"papermill":{"duration":0.020349,"end_time":"2024-02-24T15:00:22.977905","exception":false,"start_time":"2024-02-24T15:00:22.957556","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the transform, the mean is ```mean=[0.485, 0.456, 0.406]``` and ```std=[0.229, 0.224, 0.225]``` to normalize images because it is given by PyTorch by default and is the Imagenet default values and in practice it is assumed to work better then rest values.","metadata":{}},{"cell_type":"markdown","source":"The label is 0 and 1.","metadata":{}},{"cell_type":"code","source":"class dataset_new(torch.utils.data.Dataset):\n    def __init__(self,data_idx,take=0.1,path=path):\n        super().__init__()\n        classes = os.listdir(path)\n        folder_name = classes[data_idx]\n        self.images = []\n        self.labels = []\n        self.transforms = transforms.Compose([\n            transforms.Resize(256), # 256 for efficient net , 224\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n        for folder in classes:\n            if folder==folder_name:\n                limit = 1\n            else:\n                limit = take\n            anim_fold = os.path.join(path, folder)\n            for l,localpath in enumerate(os.listdir(anim_fold)):\n                if(l>=limit*len(os.listdir(anim_fold))):\n                    break\n                img_path = os.path.join(anim_fold, localpath)\n                self.images.append(img_path)\n                if folder==folder_name:\n                    self.labels.append(1)\n                else:\n                    self.labels.append(0)\n        \n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self,idx):\n        img = Image.open(self.images[idx])\n        label = self.labels[idx]\n        return self.transforms(img), torch.tensor(label,dtype=torch.long)\n      ","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:23.007061Z","iopub.status.busy":"2024-02-24T15:00:23.006793Z","iopub.status.idle":"2024-02-24T15:00:23.017338Z","shell.execute_reply":"2024-02-24T15:00:23.016469Z"},"papermill":{"duration":0.027024,"end_time":"2024-02-24T15:00:23.019119","exception":false,"start_time":"2024-02-24T15:00:22.992095","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-02-24T13:29:50.858502Z","iopub.status.busy":"2024-02-24T13:29:50.858254Z","iopub.status.idle":"2024-02-24T13:29:50.870477Z","shell.execute_reply":"2024-02-24T13:29:50.869571Z","shell.execute_reply.started":"2024-02-24T13:29:50.85848Z"},"papermill":{"duration":0.013755,"end_time":"2024-02-24T15:00:23.046754","exception":false,"start_time":"2024-02-24T15:00:23.032999","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.052106,"end_time":"2024-02-24T15:00:23.113059","exception":false,"start_time":"2024-02-24T15:00:23.060953","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```run_label_classification_one_vs_rest_fold```, this function is the main structure, and works to run the model using different models and perform calcualtions. It does 3fold CV with 3 epochs each using one_vs_rest and displays the test and train, accuracy and loss value after each epoch and also saves the plot.","metadata":{}},{"cell_type":"code","source":"def run_label_classification_one_vs_rest_fold(model_net, name):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    name_model = str(name)\n    for i in range(90):\n        print(animal_info.split()[i])\n        dataset = dataset_new(i)\n\n        kfold = KFold(n_splits=3, shuffle=True)\n\n        print('--------------------------------')\n\n        for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n\n            print(f'FOLD {fold}')\n            print('--------------------------------')\n\n            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n            test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n\n            train_loader = DataLoader(dataset, batch_size=32, sampler=train_subsampler, pin_memory=True, num_workers=2)\n            test_loader = DataLoader(dataset, batch_size=32, sampler=test_subsampler, pin_memory=True, num_workers=2)\n\n            model = model_net\n            model.to(device)\n\n            optimizer = optim.Adam(model.parameters(), lr=5e-5)\n            train_losses = []\n            test_losses = []\n            train_accuracies = []\n            test_accuracies = []\n\n            for epoch in range(3):\n\n                model.train()\n                correct = 0\n                total = 0\n                train_loss = 0.0\n                for _, (inputs, labels) in enumerate(train_loader):\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    optimizer.zero_grad()\n\n                    outputs = model(inputs)\n                    loss = nn.CrossEntropyLoss()(outputs, labels)\n                    loss.backward()\n                    optimizer.step()\n                    train_loss += loss.item()\n                    _, predicted = outputs.max(1)\n                    total += labels.size(0)\n                    correct += predicted.eq(labels).sum().item()\n\n                model.eval()  \n                test_loss = 0.0\n                test_correct = 0\n                test_total = 0\n                all_labels = []\n                all_predictions = []\n\n                for inputs, labels in test_loader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = nn.CrossEntropyLoss()(outputs, labels)\n                    test_loss += loss.item()*inputs.size(0)\n                    _, predicted = outputs.max(1)\n                    test_total += labels.size(0)\n                    test_correct += predicted.eq(labels).sum().item()\n                    all_labels.extend(labels.cpu().numpy())\n                    all_predictions.extend(predicted.cpu().numpy())\n\n\n                test_loss = test_loss / len(test_loader)\n                train_loss = train_loss/len(train_loader)\n                test_accuracy = 100.0 * correct / total\n                train_accuracy = 100.0 * test_correct/test_total\n\n                train_losses.append(train_loss)\n                test_losses.append(test_loss)\n                train_accuracies.append(train_accuracy)\n                test_accuracies.append(test_accuracy)\n\n                print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f} \\tTrain Accuracy {:.6f}% \\tTest Accuracy: {:.2f}%'.format(\n                    epoch+1, \n                    train_loss,\n                    test_loss,\n                    train_accuracy,\n                    test_accuracy\n                    ))\n                \n            directory = f'/kaggle/working/one-vs-rest/{name}/{animal_info.split()[i]}/'\n            os.makedirs(directory, exist_ok=True)\n\n            plt.figure(figsize=(10, 5))\n            plt.plot(train_accuracies, label='Train Accuracy')\n            plt.plot(test_accuracies, label='Test Accuracy')\n            plt.title('Accuracy Curve for Dataset Name: {} Fold {}'.format(animal_info.split()[i], fold))\n            plt.xlabel('Epoch')\n            plt.ylabel('Accuracy')\n            plt.savefig(os.path.join(directory, f'{fold}.png'))\n\n\n            print('Confusion Matrix for fold {}'.format(fold))\n            print(confusion_matrix(all_labels, all_predictions))","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:23.142937Z","iopub.status.busy":"2024-02-24T15:00:23.14237Z","iopub.status.idle":"2024-02-24T15:00:23.162155Z","shell.execute_reply":"2024-02-24T15:00:23.161293Z"},"papermill":{"duration":0.036922,"end_time":"2024-02-24T15:00:23.164032","exception":false,"start_time":"2024-02-24T15:00:23.12711","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.013818,"end_time":"2024-02-24T15:00:23.191918","exception":false,"start_time":"2024-02-24T15:00:23.1781","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model using Efficientnet b0","metadata":{"papermill":{"duration":0.013882,"end_time":"2024-02-24T15:00:23.2198","exception":false,"start_time":"2024-02-24T15:00:23.205918","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:23.249352Z","iopub.status.busy":"2024-02-24T15:00:23.249031Z","iopub.status.idle":"2024-02-24T15:00:38.958031Z","shell.execute_reply":"2024-02-24T15:00:38.956937Z"},"papermill":{"duration":15.726313,"end_time":"2024-02-24T15:00:38.960371","exception":false,"start_time":"2024-02-24T15:00:23.234058","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-02-23T17:15:02.244092Z","iopub.status.busy":"2024-02-23T17:15:02.243702Z","iopub.status.idle":"2024-02-23T17:15:02.257983Z","shell.execute_reply":"2024-02-23T17:15:02.257053Z","shell.execute_reply.started":"2024-02-23T17:15:02.244053Z"},"papermill":{"duration":0.014603,"end_time":"2024-02-24T15:00:38.990152","exception":false,"start_time":"2024-02-24T15:00:38.975549","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom efficientnet_pytorch import EfficientNet\n\nclass CEfficientNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n        self.fc = nn.Linear(1000, 2)\n\n    def forward(self, x):\n        x = self.efficientnet(x)\n        x = self.fc(x)\n        return x\n\nmodel_EfficientNet = CEfficientNet()\nrun_label_classification_one_vs_rest_fold(model_EfficientNet, 'Effnet')","metadata":{"execution":{"iopub.execute_input":"2024-02-24T15:00:39.02155Z","iopub.status.busy":"2024-02-24T15:00:39.020909Z","iopub.status.idle":"2024-02-24T16:11:31.289258Z","shell.execute_reply":"2024-02-24T16:11:31.288137Z"},"papermill":{"duration":4252.28673,"end_time":"2024-02-24T16:11:31.291518","exception":false,"start_time":"2024-02-24T15:00:39.004788","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.25485,"end_time":"2024-02-24T16:11:31.811724","exception":false,"start_time":"2024-02-24T16:11:31.556874","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model using MobileNet","metadata":{"papermill":{"duration":0.23828,"end_time":"2024-02-24T16:11:32.288007","exception":false,"start_time":"2024-02-24T16:11:32.049727","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nfrom torchvision import models\n\nclass CMobileNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mobilenet = models.mobilenet_v2(pretrained=True)\n        self.fc = nn.Linear(1000, 2)\n\n    def forward(self, x):\n        x = self.mobilenet(x)\n        x = self.fc(x)\n        return x\n    \nmodel_MobileNet = CMobileNet()\nrun_label_classification_one_vs_rest_fold(model_MobileNet, 'Mobilenet')","metadata":{"execution":{"iopub.execute_input":"2024-02-24T16:11:32.775712Z","iopub.status.busy":"2024-02-24T16:11:32.775316Z","iopub.status.idle":"2024-02-24T17:21:48.144685Z","shell.execute_reply":"2024-02-24T17:21:48.143659Z"},"papermill":{"duration":4215.614493,"end_time":"2024-02-24T17:21:48.147027","exception":false,"start_time":"2024-02-24T16:11:32.532534","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model using CustomNet","metadata":{"papermill":{"duration":0.498804,"end_time":"2024-02-24T17:21:49.166283","exception":false,"start_time":"2024-02-24T17:21:48.667479","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass Attention(nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.Tanh(),\n            nn.Linear(in_features, 1),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        attention_weights = self.attention(x)\n        return attention_weights * x\n\nclass CustomCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(64*56*56, 1000) \n        self.attention = Attention(1000)\n        self.fc2 = nn.Linear(1000, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1) \n        x = F.relu(self.fc1(x))\n        x = self.attention(x)\n        x = self.fc2(x)\n        return x\n\nmodel_CustomCNN = CustomCNN()\nrun_label_classification_one_vs_rest_fold(model_CustomCNN, 'CustomCNN')","metadata":{"execution":{"iopub.execute_input":"2024-02-24T17:21:50.157137Z","iopub.status.busy":"2024-02-24T17:21:50.156744Z","iopub.status.idle":"2024-02-24T18:33:10.394755Z","shell.execute_reply":"2024-02-24T18:33:10.393687Z"},"papermill":{"duration":4280.732885,"end_time":"2024-02-24T18:33:10.397104","exception":false,"start_time":"2024-02-24T17:21:49.664219","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Class Classification using 3 Fold CV","metadata":{"papermill":{"duration":0.859197,"end_time":"2024-02-24T18:33:12.028488","exception":false,"start_time":"2024-02-24T18:33:11.169291","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path = \"/kaggle/input/animal-image-dataset-90-different-animals/animals/animals\"","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:33:13.571201Z","iopub.status.busy":"2024-02-24T18:33:13.570155Z","iopub.status.idle":"2024-02-24T18:33:13.575041Z","shell.execute_reply":"2024-02-24T18:33:13.574161Z"},"papermill":{"duration":0.776609,"end_time":"2024-02-24T18:33:13.576846","exception":false,"start_time":"2024-02-24T18:33:12.800237","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset_pent_class(torch.utils.data.Dataset):\n    def __init__(self,start_idx, path=path, window_size= 5):\n        super().__init__()\n        classes = os.listdir(path)\n        self.images = []\n        self.labels = []\n        self.transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n        end_idx = start_idx*5 + window_size\n        for i in range(start_idx*5, end_idx):\n            folder_name = classes[i] \n            anim_fold = os.path.join(path, folder_name)\n            for localpath in os.listdir(anim_fold):\n                img_path = os.path.join(anim_fold, localpath)\n                self.images.append(img_path)\n                self.labels.append(i % window_size)  \n        \n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self,idx):\n        img = Image.open(self.images[idx])\n        label = self.labels[idx]\n        return self.transforms(img), torch.tensor(label, dtype=torch.long)\n      ","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:33:15.136235Z","iopub.status.busy":"2024-02-24T18:33:15.135314Z","iopub.status.idle":"2024-02-24T18:33:15.14607Z","shell.execute_reply":"2024-02-24T18:33:15.145278Z"},"papermill":{"duration":0.794446,"end_time":"2024-02-24T18:33:15.148068","exception":false,"start_time":"2024-02-24T18:33:14.353622","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```run_label_classification_one_vs_rest_fold``` even though the method name is the same, this takes multiple inputs, and saves the model weights. These weights are then loaded to visualise the model view images after the full process.","metadata":{}},{"cell_type":"code","source":"def run_label_classification_one_vs_rest_fold(model_net, name, start_idx=0, window_size=5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    name_model = str(name)\n    for i in range(start_idx, 17, window_size):\n        print(i, \"structure folder\")\n        dataset = dataset_pent_class(i, window_size=window_size)\n\n        kfold = KFold(n_splits=3, shuffle=True)\n\n        print('--------------------------------')\n\n        for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n\n            print(f'FOLD {fold}')\n            print('--------------------------------')\n\n            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n            test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n\n            train_loader = DataLoader(dataset, batch_size=32, sampler=train_subsampler, pin_memory=True, num_workers=2)\n            test_loader = DataLoader(dataset, batch_size=32, sampler=test_subsampler, pin_memory=True, num_workers=2)\n\n            model = model_net\n            model.to(device)\n\n            optimizer = optim.Adam(model.parameters(), lr=5e-5)\n            train_losses = []\n            test_losses = []\n            train_accuracies = []\n            test_accuracies = []\n\n            for epoch in range(3):\n\n                model.train()\n                correct = 0\n                total = 0\n                train_loss = 0.0\n                for _, (inputs, labels) in enumerate(train_loader):\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    optimizer.zero_grad()\n\n                    outputs = model(inputs)\n                    loss = nn.CrossEntropyLoss()(outputs, labels)\n                    loss.backward()\n                    optimizer.step()\n                    train_loss += loss.item()\n                    _, predicted = outputs.max(1)\n                    total += labels.size(0)\n                    correct += predicted.eq(labels).sum().item()\n\n                model.eval()  \n                test_loss = 0.0\n                test_correct = 0\n                test_total = 0\n                all_labels = []\n                all_predictions = []\n\n                for inputs, labels in test_loader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = nn.CrossEntropyLoss()(outputs, labels)\n                    test_loss += loss.item()*inputs.size(0)\n                    _, predicted = outputs.max(1)\n                    test_total += labels.size(0)\n                    test_correct += predicted.eq(labels).sum().item()\n                    all_labels.extend(labels.cpu().numpy())\n                    all_predictions.extend(predicted.cpu().numpy())\n\n\n                test_loss = test_loss / len(test_loader)\n                train_loss = train_loss/len(train_loader)\n                test_accuracy = 100.0 * correct / total\n                train_accuracy = 100.0 * test_correct/test_total\n\n                train_losses.append(train_loss)\n                test_losses.append(test_loss)\n                train_accuracies.append(train_accuracy)\n                test_accuracies.append(test_accuracy)\n\n                print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f} \\tTrain Accuracy {:.6f}% \\tTest Accuracy: {:.2f}%'.format(\n                    epoch+1, \n                    train_loss,\n                    test_loss,\n                    train_accuracy,\n                    test_accuracy\n                    ))\n                \n            directory = f'/kaggle/working/5fold/{i}/{fold}/'\n            os.makedirs(directory, exist_ok=True)\n\n            plt.figure(figsize=(10, 5))\n            plt.plot(train_accuracies, label='Train Accuracy')\n            plt.plot(test_accuracies, label='Test Accuracy')\n            plt.title('Accuracy Curve for {} Dataset, Fold {}'.format(i, fold))\n            plt.xlabel('Epoch')\n            plt.ylabel('Accuracy')\n            plt.savefig(os.path.join(directory, f'{fold}.png'))\n            \n            model_directory = f'/kaggle/working/5fold_model/'\n            os.makedirs(model_directory, exist_ok=True)\n            torch.save(model.state_dict(), os.path.join(model_directory, f'{name_model}.pth'))\n\n\n            print('Confusion Matrix for fold {}'.format(fold))\n            print(confusion_matrix(all_labels, all_predictions))\n","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:33:16.811875Z","iopub.status.busy":"2024-02-24T18:33:16.811474Z","iopub.status.idle":"2024-02-24T18:33:16.838412Z","shell.execute_reply":"2024-02-24T18:33:16.837448Z"},"papermill":{"duration":0.915254,"end_time":"2024-02-24T18:33:16.840424","exception":false,"start_time":"2024-02-24T18:33:15.92517","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EfficientNet b0\n","metadata":{"papermill":{"duration":0.757349,"end_time":"2024-02-24T18:33:18.370206","exception":false,"start_time":"2024-02-24T18:33:17.612857","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:33:19.983068Z","iopub.status.busy":"2024-02-24T18:33:19.982654Z","iopub.status.idle":"2024-02-24T18:33:32.093947Z","shell.execute_reply":"2024-02-24T18:33:32.092727Z"},"papermill":{"duration":12.875395,"end_time":"2024-02-24T18:33:32.096371","exception":false,"start_time":"2024-02-24T18:33:19.220976","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom efficientnet_pytorch import EfficientNet\n\nclass CEfficientNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n        self.fc = nn.Linear(1000, 5)\n\n    def forward(self, x):\n        x = self.efficientnet(x)\n        x = self.fc(x)\n        return x\n\nmodel_EfficientNet = CEfficientNet()\nrun_label_classification_one_vs_rest_fold(model_EfficientNet, 'EffNet', 0, 5)","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:33:33.62663Z","iopub.status.busy":"2024-02-24T18:33:33.626214Z","iopub.status.idle":"2024-02-24T18:35:30.924466Z","shell.execute_reply":"2024-02-24T18:35:30.923318Z"},"papermill":{"duration":118.068024,"end_time":"2024-02-24T18:35:30.926707","exception":false,"start_time":"2024-02-24T18:33:32.858683","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.777862,"end_time":"2024-02-24T18:35:32.481602","exception":false,"start_time":"2024-02-24T18:35:31.70374","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MobileNet","metadata":{"papermill":{"duration":0.773077,"end_time":"2024-02-24T18:35:34.100053","exception":false,"start_time":"2024-02-24T18:35:33.326976","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nfrom torchvision import models\n\nclass CMobileNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mobilenet = models.mobilenet_v2(pretrained=True)\n        self.fc = nn.Linear(1000, 5)\n\n    def forward(self, x):\n        x = self.mobilenet(x)\n        x = self.fc(x)\n        return x\n    \nmodel_MobileNet = CMobileNet()\nrun_label_classification_one_vs_rest_fold(model_MobileNet, 'MobileNet', 0, 5)","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:35:35.742612Z","iopub.status.busy":"2024-02-24T18:35:35.742193Z","iopub.status.idle":"2024-02-24T18:37:26.105036Z","shell.execute_reply":"2024-02-24T18:37:26.104065Z"},"papermill":{"duration":111.181573,"end_time":"2024-02-24T18:37:26.10719","exception":false,"start_time":"2024-02-24T18:35:34.925617","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.778811,"end_time":"2024-02-24T18:37:27.667799","exception":false,"start_time":"2024-02-24T18:37:26.888988","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom NeuralNetwork","metadata":{"papermill":{"duration":0.988864,"end_time":"2024-02-24T18:37:29.441032","exception":false,"start_time":"2024-02-24T18:37:28.452168","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n\nclass Attention(nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.Tanh(),\n            nn.Linear(in_features, 1),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        attention_weights = self.attention(x)\n        return attention_weights * x\n    \nclass CustomCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(64*56*56, 1000) \n        self.attention = Attention(1000)\n        self.fc2 = nn.Linear(1000, 5) \n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1) \n        x = F.relu(self.fc1(x))\n        x = self.attention(x)\n        x = self.fc2(x)\n        return x\n\nmodel_CustomCNN = CustomCNN()\nrun_label_classification_one_vs_rest_fold(model_CustomCNN, 'CustomCNN', 0, 5)","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:37:30.999044Z","iopub.status.busy":"2024-02-24T18:37:30.998137Z","iopub.status.idle":"2024-02-24T18:39:48.267146Z","shell.execute_reply":"2024-02-24T18:39:48.266107Z"},"papermill":{"duration":138.050488,"end_time":"2024-02-24T18:39:48.269337","exception":false,"start_time":"2024-02-24T18:37:30.218849","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.955671,"end_time":"2024-02-24T18:39:50.052944","exception":false,"start_time":"2024-02-24T18:39:49.097273","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Visualisation ","metadata":{"papermill":{"duration":0.851534,"end_time":"2024-02-24T18:39:51.754653","exception":false,"start_time":"2024-02-24T18:39:50.903119","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torchvision\nfrom torchvision import models, transforms, utils\nfrom torch.autograd import Variable\nimport scipy.misc\nimport json","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:39:53.429566Z","iopub.status.busy":"2024-02-24T18:39:53.42914Z","iopub.status.idle":"2024-02-24T18:39:53.439204Z","shell.execute_reply":"2024-02-24T18:39:53.438303Z"},"papermill":{"duration":0.839727,"end_time":"2024-02-24T18:39:53.441429","exception":false,"start_time":"2024-02-24T18:39:52.601702","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:39:55.124032Z","iopub.status.busy":"2024-02-24T18:39:55.122845Z","iopub.status.idle":"2024-02-24T18:39:55.129079Z","shell.execute_reply":"2024-02-24T18:39:55.128194Z"},"papermill":{"duration":0.849538,"end_time":"2024-02-24T18:39:55.131286","exception":false,"start_time":"2024-02-24T18:39:54.281748","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the same transform as used during dataloading","metadata":{"papermill":{"duration":0.873119,"end_time":"2024-02-24T18:39:56.829031","exception":false,"start_time":"2024-02-24T18:39:55.955912","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Plotting the output of all convolutional layers and discuss the insights on automatically created features.","metadata":{"papermill":{"duration":0.927076,"end_time":"2024-02-24T18:39:58.594372","exception":false,"start_time":"2024-02-24T18:39:57.667296","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Taking an Image\nimage = Image.open(str('/kaggle/input/animal-image-dataset-90-different-animals/animals/animals/antelope/02f4b3be2d.jpg'))\nplt.imshow(image)\n","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:40:00.264311Z","iopub.status.busy":"2024-02-24T18:40:00.263359Z","iopub.status.idle":"2024-02-24T18:40:00.91952Z","shell.execute_reply":"2024-02-24T18:40:00.91853Z"},"papermill":{"duration":1.510591,"end_time":"2024-02-24T18:40:00.924832","exception":false,"start_time":"2024-02-24T18:39:59.414241","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function for Visual\n","metadata":{"papermill":{"duration":0.841725,"end_time":"2024-02-24T18:40:02.603585","exception":false,"start_time":"2024-02-24T18:40:01.76186","status":"completed"},"tags":[]}},{"cell_type":"code","source":"transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n\ndef nn_layer_visual(image, model, model_path):\n        \n        model.load_state_dict(torch.load(model_path))\n\n        \n        model_weights =[]\n        conv_layers = []\n        model_children = list(model.children())\n        counter = 0\n        for i in range(len(model_children)):\n            if type(model_children[i]) == nn.Conv2d:\n                counter+=1\n                model_weights.append(model_children[i].weight)\n                conv_layers.append(model_children[i])\n            elif type(model_children[i]) == nn.Sequential:\n                for j in range(len(model_children[i])):\n                    for child in model_children[i][j].children():\n                        if type(child) == nn.Conv2d:\n                            counter+=1\n                            model_weights.append(child.weight)\n                            conv_layers.append(child)\n        print(f\"Total convolution layers: {counter}\")\n        print(\"conv_layers\")\n\n        image = transforms(image)\n        print(f\"Image shape before: {image.shape}\")\n        image = image.unsqueeze(0)\n        print(f\"Image shape after: {image.shape}\")\n\n        if len(image.shape) == 3:\n            image = image.unsqueeze(0)\n\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        image = image.to(device)\n\n        print(f\"Image shape before: {image.shape}\")\n\n        outputs = []\n        names = []\n        for layer in conv_layers[0:]:\n            image = layer(image)\n            outputs.append(image)\n            names.append(str(layer))\n\n        print(len(outputs))\n        for feature_map in outputs:\n            print(feature_map.shape)\n\n        processed = []\n        for feature_map in outputs:\n            feature_map = feature_map.squeeze(0)\n            gray_scale = torch.sum(feature_map,0)\n            gray_scale = gray_scale / feature_map.shape[0]\n            processed.append(gray_scale.data.cpu().numpy())\n        for fm in processed:\n            print(fm.shape)\n\n        fig = plt.figure(figsize=(30, 50))\n        for i in range(len(processed)):\n            a = fig.add_subplot(5, 4, i+1)\n            imgplot = plt.imshow(processed[i])\n            a.axis(\"off\")\n            a.set_title(names[i].split('(')[0], fontsize=30)\n","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:40:04.246177Z","iopub.status.busy":"2024-02-24T18:40:04.245685Z","iopub.status.idle":"2024-02-24T18:40:04.263684Z","shell.execute_reply":"2024-02-24T18:40:04.262739Z"},"papermill":{"duration":0.84196,"end_time":"2024-02-24T18:40:04.265756","exception":false,"start_time":"2024-02-24T18:40:03.423796","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom CNN Visual","metadata":{"papermill":{"duration":0.83987,"end_time":"2024-02-24T18:40:05.930562","exception":false,"start_time":"2024-02-24T18:40:05.090692","status":"completed"},"tags":[]}},{"cell_type":"code","source":"nn_layer_visual(image, model_CustomCNN, '/kaggle/working/5fold_model/CustomCNN.pth')","metadata":{"execution":{"iopub.execute_input":"2024-02-24T18:40:07.639621Z","iopub.status.busy":"2024-02-24T18:40:07.639081Z","iopub.status.idle":"2024-02-24T18:40:09.020143Z","shell.execute_reply":"2024-02-24T18:40:09.019193Z"},"papermill":{"duration":2.196922,"end_time":"2024-02-24T18:40:09.028022","exception":false,"start_time":"2024-02-24T18:40:06.8311","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.816762,"end_time":"2024-02-24T18:40:10.684487","exception":false,"start_time":"2024-02-24T18:40:09.867725","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.833926,"end_time":"2024-02-24T18:40:12.336741","exception":false,"start_time":"2024-02-24T18:40:11.502815","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.835225,"end_time":"2024-02-24T18:40:14.000055","exception":false,"start_time":"2024-02-24T18:40:13.16483","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}